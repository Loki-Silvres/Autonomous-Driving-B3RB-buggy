# Autonomous Driving B3RB-buggy
The B3RB Autonomous Driving project represents a comprehensive exploration of robotic perception and autonomous navigation, developed as part of the NXP-AIM'24 challenge. Our team successfully created an intelligent robotic platform that demonstrates advanced capabilities in real-time environmental interpretation and autonomous decision-making.

![lidar_camera_testing](https://github.com/user-attachments/assets/fc350911-0ca5-4cb4-ba40-039bcd5ae803)
![model_testing](https://github.com/user-attachments/assets/480f7d78-9e2b-4fca-a019-b8c6555ca41b)

## Technical Architecture

The autonomous system was built on a robust technological stack, integrating multiple advanced perception and control mechanisms:

- **Computational Platform**: Mini computer running ROS2 on Ubuntu 22
- **Perception Systems**:
   . - LIDAR for environmental mapping
   . - Camera for lane detection and traffic sign recognition
- **Control Mechanism**: Ackermann-steering control for precise navigation
- **Machine Learning Model**: YOLOv5s with INT8 quantization

## Key Achievements

- **Performance Milestone**: Achieved an impressive 1:42 track time
- **Real-time Inference**: Maintained 7 Hz processing speed
- **Advanced Perception**: Implemented comprehensive obstacle detection and lane tracking
- **Optimization**: Utilized NPU-optimized model for efficient processing

[![Simulation_Demo](https://img.youtube.com/vi/n6aP2X9CODE/0.jpg)](https://www.youtube.com/watch?v=n6aP2X9CODE)

![b3rb_simulation](https://github.com/user-attachments/assets/b5857c8c-7de6-4da4-84dd-2e5f32d7caf1)
![buggy_on_track](https://github.com/user-attachments/assets/29b5d33a-2619-4eeb-b329-a96a61cd8725)
![buggy_block-diagram](https://github.com/user-attachments/assets/79ff9544-b66c-417b-8f12-89299ae33800)

---
 
## Note
The grand finale videos are unavailable due to NXP-Semiconductors office regulations.

---    
